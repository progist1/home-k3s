apiVersion: apps/v1
kind: Deployment
metadata:
  name: tdarr-node
  namespace: apps
spec:
  replicas: 3
  selector:
    matchLabels:
      app: tdarr-node
  template:
    metadata:
      labels:
        app: tdarr-node
    spec:
      # Размещение ТОЛЬКО на GPU-ноде (Intel i915 / QSV); несколько подов делят GPU через sharedDevNum
      nodeSelector:
        node.hardware.gpu: intel-igpu
        node.workload.class: gpu
      tolerations:
        - key: node.hardware.gpu
          operator: Equal
          value: intel-igpu
          effect: NoSchedule

      # Инициализация под-специфичных каталогов на общих PVC (configs, logs, temp)
      initContainers:
        - name: ensure-dirs
          image: busybox:1.36
          command:
            - sh
            - -c
            - |
              mkdir -p /data/configs/$(POD_NAME) /data/logs/$(POD_NAME) /temp/$(POD_NAME)
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          volumeMounts:
            - name: data
              mountPath: /data
            - name: temp
              mountPath: /temp

      # securityContext не задаём — образ использует s6-applyuidgid, иначе "unable to set supplementary group list: Operation not permitted"
      containers:
        - name: tdarr-node
          image: ghcr.io/haveagitgat/tdarr_node:2.58.02
          env:
            - name: TZ
              value: Europe/Moscow
            - name: inContainer
              value: "true"
            # Уникальное имя ноды на реплику (Tdarr Server различает ноды по имени)
            - name: nodeName
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            # Подключение к Tdarr Server в кластере (порт 8266 — для нод)
            - name: serverURL
              value: "http://tdarr.apps.svc.cluster.local:8266"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          ports:
            - containerPort: 8267
          volumeMounts:
            - name: data
              mountPath: /app/configs
              subPathExpr: configs/$(POD_NAME)
            - name: data
              mountPath: /app/logs
              subPathExpr: logs/$(POD_NAME)
            - name: temp
              mountPath: /temp
              subPathExpr: $(POD_NAME)
            - name: media
              mountPath: /media
          resources:
            requests:
              cpu: "200m"
              memory: "512Mi"
              gpu.intel.com/i915: "1"
            limits:
              cpu: "4"
              memory: "4Gi"
              gpu.intel.com/i915: "1"
          livenessProbe:
            # Node не слушает порты (только исходящее подключение к серверу); проверяем процесс
            exec:
              command: ["pgrep", "-x", "Tdarr_Node"]
            initialDelaySeconds: 90
            periodSeconds: 30
          readinessProbe:
            exec:
              command: ["pgrep", "-x", "Tdarr_Node"]
            initialDelaySeconds: 45
            periodSeconds: 15

      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: tdarr-node-data-pvc
        - name: temp
          persistentVolumeClaim:
            claimName: tdarr-node-temp-pvc
        - name: media
          persistentVolumeClaim:
            claimName: tdarr-server-media-pvc
